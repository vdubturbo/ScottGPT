-- ======================================================================
-- ScottGPT pgvector Setup - Complete SQL Commands
-- ======================================================================
-- 
-- Run these commands in your Supabase SQL Editor to enable pgvector:
-- https://app.supabase.com/project/[YOUR_PROJECT]/sql
-- 
-- Step 1: Copy and paste ALL commands below into the SQL editor
-- Step 2: Click "Run" to execute all commands
-- Step 3: Run: node verify-pgvector.js to confirm setup
-- ======================================================================

-- Enable pgvector extension
CREATE EXTENSION IF NOT EXISTS vector;

-- Add embedding_vector column to content_chunks table
ALTER TABLE content_chunks ADD COLUMN IF NOT EXISTS embedding_vector vector(1024);

-- Create function to convert JSON embeddings to vector format
CREATE OR REPLACE FUNCTION json_array_to_vector(embedding_json TEXT)
RETURNS vector(1024) AS $$
DECLARE
  parsed_array FLOAT[];
  vector_result vector(1024);
BEGIN
  IF embedding_json IS NULL OR embedding_json = '' THEN
    RETURN NULL;
  END IF;
  
  SELECT array_agg(value::FLOAT) 
  INTO parsed_array
  FROM json_array_elements_text(embedding_json::json);
  
  IF array_length(parsed_array, 1) != 1024 THEN
    RAISE WARNING 'Invalid embedding dimension: expected 1024, got %', array_length(parsed_array, 1);
    RETURN NULL;
  END IF;
  
  vector_result := parsed_array::vector(1024);
  
  RETURN vector_result;
EXCEPTION WHEN OTHERS THEN
  RAISE WARNING 'Failed to convert embedding: %', SQLERRM;
  RETURN NULL;
END;
$$ LANGUAGE plpgsql;

-- Create migration function to populate vector column from existing embeddings
CREATE OR REPLACE FUNCTION migrate_embeddings_to_vector()
RETURNS TABLE(processed INTEGER, failed INTEGER) AS $$
DECLARE
  chunk_record RECORD;
  processed_count INTEGER := 0;
  failed_count INTEGER := 0;
BEGIN
  FOR chunk_record IN 
    SELECT id, embedding 
    FROM content_chunks 
    WHERE embedding IS NOT NULL 
    AND embedding_vector IS NULL
  LOOP
    BEGIN
      UPDATE content_chunks 
      SET embedding_vector = json_array_to_vector(chunk_record.embedding)
      WHERE id = chunk_record.id;
      
      processed_count := processed_count + 1;
      
      IF processed_count % 100 = 0 THEN
        RAISE NOTICE 'Processed % embeddings...', processed_count;
      END IF;
      
    EXCEPTION WHEN OTHERS THEN
      failed_count := failed_count + 1;
      RAISE WARNING 'Failed to convert embedding for chunk ID %: %', chunk_record.id, SQLERRM;
    END;
  END LOOP;
  
  RETURN QUERY SELECT processed_count, failed_count;
END;
$$ LANGUAGE plpgsql;

-- Create optimized vector similarity search function
CREATE OR REPLACE FUNCTION fast_similarity_search(
  query_embedding vector(1024),
  similarity_threshold FLOAT DEFAULT 0.5,
  max_results INTEGER DEFAULT 20,
  filter_skills TEXT[] DEFAULT NULL,
  filter_tags TEXT[] DEFAULT NULL,
  date_after TEXT DEFAULT NULL,
  date_before TEXT DEFAULT NULL
)
RETURNS TABLE(
  id INTEGER,
  source_id TEXT,
  title TEXT,
  content TEXT,
  content_summary TEXT,
  skills TEXT[],
  tags TEXT[],
  date_start TEXT,
  date_end TEXT,
  similarity FLOAT,
  source_type TEXT,
  source_title TEXT,
  source_org TEXT,
  source_location TEXT
) AS $$
BEGIN
  RETURN QUERY
  SELECT 
    cc.id,
    cc.source_id,
    cc.title,
    cc.content,
    cc.content_summary,
    cc.skills,
    cc.tags,
    cc.date_start,
    cc.date_end,
    (1 - (cc.embedding_vector <=> query_embedding)) AS similarity,
    s.type AS source_type,
    s.title AS source_title,
    s.org AS source_org,
    s.location AS source_location
  FROM content_chunks cc
  LEFT JOIN sources s ON cc.source_id = s.id
  WHERE 
    cc.embedding_vector IS NOT NULL
    AND (1 - (cc.embedding_vector <=> query_embedding)) >= similarity_threshold
    AND (filter_skills IS NULL OR cc.skills && filter_skills)
    AND (filter_tags IS NULL OR cc.tags && filter_tags)
    AND (date_after IS NULL OR cc.date_start >= date_after)
    AND (date_before IS NULL OR cc.date_end <= date_before)
  ORDER BY cc.embedding_vector <=> query_embedding
  LIMIT max_results;
END;
$$ LANGUAGE plpgsql;

-- Create performance monitoring function
CREATE OR REPLACE FUNCTION get_vector_search_stats()
RETURNS TABLE(
  total_chunks INTEGER,
  chunks_with_vectors INTEGER,
  vector_column_size TEXT,
  index_count INTEGER
) AS $$
BEGIN
  SELECT COUNT(*) INTO total_chunks FROM content_chunks;
  SELECT COUNT(*) INTO chunks_with_vectors FROM content_chunks WHERE embedding_vector IS NOT NULL;
  SELECT pg_size_pretty(pg_total_relation_size('content_chunks')) INTO vector_column_size;
  SELECT COUNT(*) INTO index_count 
  FROM pg_indexes 
  WHERE tablename = 'content_chunks' 
  AND indexdef LIKE '%embedding_vector%';
  
  RETURN QUERY SELECT total_chunks, chunks_with_vectors, vector_column_size, index_count;
END;
$$ LANGUAGE plpgsql;

-- Run the migration immediately after setup
SELECT * FROM migrate_embeddings_to_vector();

-- Create index for fast similarity search (this may take a few minutes)
CREATE INDEX IF NOT EXISTS content_chunks_embedding_vector_idx 
ON content_chunks 
USING hnsw (embedding_vector vector_cosine_ops)
WITH (m = 16, ef_construction = 64);

-- Display setup results
SELECT 
  'Setup Status' as component,
  CASE WHEN COUNT(*) > 0 THEN 'COMPLETE' ELSE 'FAILED' END as status
FROM pg_extension WHERE extname = 'vector';

-- Show migration results
SELECT * FROM get_vector_search_stats();

-- Instructions message
DO $$ 
BEGIN 
  RAISE NOTICE '';
  RAISE NOTICE '============================================';
  RAISE NOTICE 'pgvector Setup Complete!';
  RAISE NOTICE '============================================';
  RAISE NOTICE '';
  RAISE NOTICE 'Next steps:';
  RAISE NOTICE '1. Run: node verify-pgvector.js';
  RAISE NOTICE '2. Expected improvement: 200-500ms â†’ 5-10ms queries';
  RAISE NOTICE '3. ScottGPT will automatically use pgvector now';
  RAISE NOTICE '';
END $$;